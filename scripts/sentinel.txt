--- File: __init__.py ---
# This file makes 'sentinel' a Python package. 
--- End File: {relative_path} ---

--- File: alert_bot.py ---
#!/usr/bin/env python3
import ccxt
import time
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import os
from dotenv import load_dotenv
import argparse
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import json
import yaml
from pathlib import Path
import logging
import statistics # Added for volatility calculation

# Setup logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Ensure logs directory exists
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)
log_file_path = log_dir / 'alert_logs.log'

# File handler
fh = logging.FileHandler(log_file_path)
fh.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
fh.setFormatter(formatter)
logger.addHandler(fh)

# Console handler
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
ch.setFormatter(formatter)
logger.addHandler(ch)

# Load environment variables
load_dotenv()

N_VOLATILITY_PERIODS = 14  # Number of periods for volatility calculation (e.g., stdev)
MIN_CANDLES_FOR_PCT_CHANGE = 2

class PriceAlert:
    def __init__(self, config_data: Dict[str, Any], main_loop_interval_seconds: int = 60):
        self.exchange = ccxt.coinbase()
        try:
            self.exchange.load_markets()
        except Exception as e:
            logger.error(f"Error loading markets: {e}. Price formatting might be affected.")

        self.config_data = config_data
        self.symbols = list(self.config_data.keys())
        if not self.symbols:
            logger.warning("No symbols found in the configuration data.")
        
        self.main_loop_interval_seconds = main_loop_interval_seconds
        
        self.ohlcv_data: Dict[str, Dict[str, List[list]]] = {symbol: {} for symbol in self.symbols}
        self.last_ohlcv_fetch_time: Dict[str, Dict[str, datetime]] = {symbol: {} for symbol in self.symbols}
        self.last_ticker_prices: Dict[str, float] = {}
        
        # Email configuration
        self.smtp_server = os.getenv('SMTP_SERVER', 'smtp.gmail.com')
        self.smtp_port = int(os.getenv('SMTP_PORT', 587))
        self.smtp_username = os.getenv('SMTP_USERNAME')
        self.smtp_password = os.getenv('SMTP_PASSWORD')
        self.alert_email = os.getenv('ALERT_EMAIL')

        logger.info(f"PriceAlert initialized for symbols: {', '.join(self.symbols)}")
        logger.info(f"Main loop interval: {self.main_loop_interval_seconds} seconds")

    def _get_ccxt_timeframe(self, minutes: int) -> Optional[str]:
        """Converts minutes to ccxt timeframe string."""
        if minutes < 1:
            return None
        if minutes < 60:
            return f"{minutes}m"
        elif minutes < 1440: # Less than a day
            hours = minutes // 60
            return f"{hours}h"
        else: # Days
            days = minutes // 1440
            return f"{days}d"

    def _format_price(self, symbol: str, price: float) -> str:
        """Formats the price according to the symbol's precision."""
        try:
            if hasattr(self.exchange, 'markets') and self.exchange.markets and symbol in self.exchange.markets and self.exchange.markets[symbol]:
                precision_info = self.exchange.markets[symbol].get('precision')
                if precision_info and 'price' in precision_info:
                    price_precision = precision_info['price']
                    if price_precision is not None:
                        if isinstance(price_precision, float): # e.g., 0.00000001 for PEPE/USD
                            s = format(price_precision, '.10f') # Format to avoid scientific notation for small floats
                            if '.' in s:
                                decimal_places = len(s.split('.')[1].rstrip('0'))
                                return f"{price:.{decimal_places}f}"
                            else: # Should not happen for float precision if it has decimals
                                return f"{price:.2f}" # fallback
                        else: # Integer precision (number of decimal places)
                            return f"{price:.{int(price_precision)}f}"
            # Fallback if market data or precision is not available
            if abs(price) > 0 and abs(price) < 0.001: # For very small prices like PEPE
                 return f"{price:.8f}" 
            return f"{price:.2f}"
        except Exception as e:
            logger.error(f"Error formatting price for {symbol} ({price}): {e}. Defaulting to .2f")
            return f"{price:.2f}"

    def send_email_alert(self, message: str):
        """Send email alert"""
        if not all([self.smtp_username, self.smtp_password, self.alert_email]):
            logger.warning("Email configuration not complete. Skipping email alert.")
            return

        try:
            msg = MIMEMultipart()
            msg['From'] = self.smtp_username
            msg['To'] = self.alert_email
            msg['Subject'] = "Crypto Price Alert"
            
            msg.attach(MIMEText(message, 'plain'))
            
            server = smtplib.SMTP(self.smtp_server, self.smtp_port)
            server.starttls()
            server.login(self.smtp_username, self.smtp_password)
            server.send_message(msg)
            server.quit()
            logger.info("Alert email sent successfully!")
        except Exception as e:
            logger.error(f"Failed to send email alert: {e}")

    def send_test_alert(self):
        """Send a test email to verify credentials"""
        test_message = f"Test Alert\\n\\nThis is a test message to verify your email configuration is working correctly.\\n\\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        logger.info("Sending test email...")
        self.send_email_alert(test_message)

    def check_percentage_change(self, symbol: str, current_price: float, alert_config: Dict[str, Any], ohlcv_candles: List[list]) -> Optional[str]:
        """Check for significant percentage changes using OHLCV data."""
        if alert_config.get('triggered_session', False):
            return None # Already triggered in this session

        required_candles = MIN_CANDLES_FOR_PCT_CHANGE
        if len(ohlcv_candles) < required_candles:
            logger.debug(f"Not enough candles for {symbol} {alert_config.get('timeframe')}m percentage change (need {required_candles}, got {len(ohlcv_candles)})")
            return None

        # OHLCV candles are [timestamp, open, high, low, close, volume]
        # Last candle (ohlcv_candles[-1]) is the current, possibly incomplete one.
        # Second to last candle (ohlcv_candles[-2]) is the most recent fully completed one.
        # Its closing price is our 'old_price'.
        old_price = ohlcv_candles[-2][4] # Index 4 is close price

        if old_price == 0: # Avoid division by zero
            logger.warning(f"Old price is zero for {symbol}, cannot calculate percentage change.")
            return None

        percentage_diff = ((current_price - old_price) / old_price) * 100
        target_percentage = alert_config['percentage']
        timeframe_minutes = alert_config['timeframe']

        # Check if absolute percentage change meets the threshold
        if abs(percentage_diff) >= target_percentage:
            direction = "risen" if percentage_diff > 0 else "fallen"
            message = (
                f"{symbol} has {direction} by {percentage_diff:.2f}% in the last {timeframe_minutes} minutes. "
                f"(From {self._format_price(symbol, old_price)} to {self._format_price(symbol, current_price)})"
            )
            alert_config['triggered_session'] = True # Mark as triggered for this session
            return message
        return None

    def check_volatility(self, symbol: str, current_price: float, alert_config: Dict[str, Any], ohlcv_candles: List[list]) -> Optional[str]:
        """Check for high volatility using OHLCV data (std dev of closing prices)."""
        if alert_config.get('triggered_session', False):
            return None # Already triggered in this session

        if len(ohlcv_candles) < N_VOLATILITY_PERIODS:
            logger.debug(f"Not enough candles for {symbol} {alert_config.get('timeframe')}m volatility (need {N_VOLATILITY_PERIODS}, got {len(ohlcv_candles)})")
            return None

        # Use closing prices of the last N_VOLATILITY_PERIODS candles
        relevant_candles = ohlcv_candles[-N_VOLATILITY_PERIODS:]
        closes = [c[4] for c in relevant_candles] # Index 4 is close price

        if not closes or len(closes) < 2: # statistics.stdev needs at least 2 data points
            logger.debug(f"Not enough closing prices for stdev calculation for {symbol}.")
            return None
        
        std_dev = statistics.stdev(closes)
        
        if current_price == 0: # Avoid division by zero
            logger.warning(f"Current price is zero for {symbol}, cannot calculate volatility percentage.")
            return None

        # Volatility as a percentage of the current price
        volatility_percentage = (std_dev / current_price) * 100
        target_threshold = alert_config['threshold']
        timeframe_minutes = alert_config['timeframe']

        if volatility_percentage >= target_threshold:
            message = (
                f"{symbol} shows high volatility of {volatility_percentage:.2f}% over the last ~{timeframe_minutes * N_VOLATILITY_PERIODS / 60:.1f} hours (based on {N_VOLATILITY_PERIODS} periods of {timeframe_minutes}m candles). "
                f"(StdDev: {self._format_price(symbol, std_dev)}, Current Price: {self._format_price(symbol, current_price)})"
            )
            # The timeframe description in the message can be a bit tricky if timeframe_minutes varies.
            # For now, this gives an idea of the total period length used for stdev calc.
            alert_config['triggered_session'] = True # Mark as triggered for this session
            return message
        return None

    def check_alerts(self, symbol: str, current_price: float):
        """Check if any price alerts have been triggered based on config_data."""
        symbol_config = self.config_data.get(symbol)
        if not symbol_config:
            return

        alerts_triggered_messages = []

        # 1. Price Level Alerts
        if 'price_levels' in symbol_config:
            for alert_conf in symbol_config['price_levels']:
                price_level = float(alert_conf['price'])
                condition = alert_conf['condition']
                # Basic check; consider adding state to prevent re-alerting immediately
                if condition == 'above' and current_price > price_level:
                    message = f"{symbol} is now above {self._format_price(symbol, price_level)} (Current: {self._format_price(symbol, current_price)})"
                    alerts_triggered_messages.append(message)
                elif condition == 'below' and current_price < price_level:
                    message = f"{symbol} is now below {self._format_price(symbol, price_level)} (Current: {self._format_price(symbol, current_price)})"
                    alerts_triggered_messages.append(message)
        
        # 2. Percentage Change Alerts (OHLCV based - Stage 2)
        if 'percentage_changes' in symbol_config:
            for alert_conf in symbol_config['percentage_changes']:
                # OHLCV fetching and checking logic will go here in Stage 2
                # For now, it will call the placeholder check_percentage_change
                timeframe_minutes = alert_conf.get('timeframe')
                ccxt_tf = self._get_ccxt_timeframe(timeframe_minutes) if timeframe_minutes else None
                if ccxt_tf and symbol in self.ohlcv_data and ccxt_tf in self.ohlcv_data[symbol]:
                    candles = self.ohlcv_data[symbol][ccxt_tf]
                    if candles:
                        msg = self.check_percentage_change(symbol, current_price, alert_conf, candles)
                        if msg: alerts_triggered_messages.append(msg)
                    else:
                        logger.debug(f"No OHLCV candles available for {symbol} {ccxt_tf} to check percentage_changes.")
                else:
                    logger.debug(f"OHLCV data not ready for {symbol} {ccxt_tf} for percentage_changes (or no timeframe specified).")

        # 3. Volatility Alerts (OHLCV based - Stage 2)
        if 'volatility' in symbol_config:
            for alert_conf in symbol_config['volatility']:
                # OHLCV fetching and checking logic will go here in Stage 2
                timeframe_minutes = alert_conf.get('timeframe')
                ccxt_tf = self._get_ccxt_timeframe(timeframe_minutes) if timeframe_minutes else None
                if ccxt_tf and symbol in self.ohlcv_data and ccxt_tf in self.ohlcv_data[symbol]:
                    candles = self.ohlcv_data[symbol][ccxt_tf]
                    if candles:
                        msg = self.check_volatility(symbol, current_price, alert_conf, candles)
                        if msg: alerts_triggered_messages.append(msg)
                    else:
                        logger.debug(f"No OHLCV candles available for {symbol} {ccxt_tf} to check volatility.")
                else:
                    logger.debug(f"OHLCV data not ready for {symbol} {ccxt_tf} for volatility (or no timeframe specified).")

        for msg in alerts_triggered_messages:
            logger.info(f"ALERT: {msg}") # Log the alert locally
            self.send_email_alert(msg)

    def _fetch_ohlcv_if_needed(self, symbol: str, alert_configs: List[Dict[str, Any]]):
        """Fetches OHLCV data for the symbol if required by any alert and if cooldown passed."""
        required_timeframes_minutes = set()
        for alert_conf in alert_configs:
            if 'timeframe' in alert_conf:
                required_timeframes_minutes.add(alert_conf['timeframe'])

        for tf_minutes in required_timeframes_minutes:
            ccxt_tf = self._get_ccxt_timeframe(tf_minutes)
            if not ccxt_tf: continue

            now = datetime.now()
            last_fetch = self.last_ohlcv_fetch_time.get(symbol, {}).get(ccxt_tf)
            # Fetch if never fetched, or if (time since last fetch >= timeframe duration)
            # Example: for a '5m' timeframe, fetch every 5 minutes.
            should_fetch = False
            if last_fetch is None:
                should_fetch = True
            else:
                if (now - last_fetch) >= timedelta(minutes=tf_minutes):
                    should_fetch = True
            
            if should_fetch:
                try:
                    # Determine number of candles. For now, a fixed number.
                    # For percentage change, 2 might be enough (current forming, last closed).
                    # For volatility, more might be needed (e.g., 20-50).
                    # Let's use a heuristic or make it configurable later.
                    limit = 50 # Default limit, can be refined per alert type
                    logger.info(f"Fetching {limit} OHLCV candles for {symbol} ({ccxt_tf})...")
                    candles = self.exchange.fetch_ohlcv(symbol, timeframe=ccxt_tf, limit=limit)
                    if symbol not in self.ohlcv_data: self.ohlcv_data[symbol] = {}
                    self.ohlcv_data[symbol][ccxt_tf] = candles
                    if symbol not in self.last_ohlcv_fetch_time: self.last_ohlcv_fetch_time[symbol] = {}
                    self.last_ohlcv_fetch_time[symbol][ccxt_tf] = now
                    logger.info(f"Fetched {len(candles)} OHLCV candles for {symbol} ({ccxt_tf}).")
                except Exception as e:
                    logger.error(f"Error fetching OHLCV for {symbol} ({ccxt_tf}): {e}")
                    # Ensure old data isn't used if fetch fails, or handle staleness
                    if symbol in self.ohlcv_data and ccxt_tf in self.ohlcv_data[symbol]:
                        del self.ohlcv_data[symbol][ccxt_tf] # Invalidate old data on error

    def run(self):
        """Main loop to fetch prices and check alerts"""
        logger.info(f"Starting price monitoring for {', '.join(self.symbols)}")
        logger.info("Press Ctrl+C to stop")
        
        if not self.symbols:
            logger.warning("No symbols to monitor. Exiting.")
            return

        while True:
            try:
                for symbol in self.symbols:
                    symbol_config = self.config_data.get(symbol)
                    if not symbol_config: continue

                    try:
                        # Fetch current ticker price
                        ticker = self.exchange.fetch_ticker(symbol)
                        current_price = ticker['last']
                        self.last_ticker_prices[symbol] = current_price
                        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        logger.info(f"[{timestamp}] {symbol}: {self._format_price(symbol, current_price)}")
                        
                        # Determine all alert configs that need OHLCV for this symbol
                        ohlcv_alert_configs = []
                        if 'percentage_changes' in symbol_config:
                            ohlcv_alert_configs.extend(symbol_config['percentage_changes'])
                        if 'volatility' in symbol_config:
                            ohlcv_alert_configs.extend(symbol_config['volatility'])
                        
                        if ohlcv_alert_configs:
                            self._fetch_ohlcv_if_needed(symbol, ohlcv_alert_configs)
                        
                        # Perform all checks for the symbol
                        self.check_alerts(symbol, current_price)
                    
                    except ccxt.NetworkError as e:
                        logger.error(f"Network error for {symbol}: {e}. Will retry.")
                    except ccxt.ExchangeError as e:
                        logger.error(f"Exchange error for {symbol}: {e}. Will retry.")
                    except Exception as e:
                        logger.error(f"Error processing symbol {symbol}: {e}")
                
                logger.debug(f"Main loop completed. Sleeping for {self.main_loop_interval_seconds}s.")
                time.sleep(self.main_loop_interval_seconds)
                
            except KeyboardInterrupt:
                logger.info("Monitoring stopped by user (Ctrl+C).")
                break
            except Exception as e:
                logger.error(f"Unhandled error in main loop: {e}", exc_info=True)
                time.sleep(60)  # Wait a minute before retrying

def create_example_config():
    """Create an example configuration file"""
    example_config = {
        'BTC/USD': {
            'price_levels': [
                {'price': 50000, 'condition': 'above'},
                {'price': 45000, 'condition': 'below'}
            ],
            'percentage_changes': [ # Example structure for future use
                {'percentage': 5, 'timeframe': 10} # 10 minutes
            ],
            'volatility': [ # Example structure for future use
                {'threshold': 2, 'timeframe': 5} # 5 minutes
            ]
        },
        'ETH/USD': {
            'price_levels': [
                {'price': 3000, 'condition': 'above'},
                {'price': 2500, 'condition': 'below'}
            ],
            'percentage_changes': [
                {'percentage': 7, 'timeframe': 15}
            ],
            'volatility': [
                {'threshold': 3, 'timeframe': 5}
            ]
        },
        'PEPE/USD': { # Example for high precision
            'price_levels': [
                {'price': 0.00001700, 'condition': 'above'},
                {'price': 0.00001200, 'condition': 'below'}
            ]
        }
    }
    
    config_path = Path('scripts/alerts_config.yaml')
    with open(config_path, 'w') as f:
        yaml.dump(example_config, f, default_flow_style=False)
    logger.info(f"Created example configuration file at {config_path}")

def main():
    parser = argparse.ArgumentParser(description='Crypto Price Alert Monitor - Config Driven')
    parser.add_argument('--config', type=str, default='scripts/alerts_config.yaml', help='Path to YAML configuration file (default: scripts/alerts_config.yaml)')
    parser.add_argument('--main-loop-interval', type=int, default=60, help='Main loop check interval in seconds (default: 60)')
    parser.add_argument('--create-config', action='store_true', help='Create an example configuration file and exit')
    parser.add_argument('--test-email', action='store_true', help='Send a test email to verify credentials and exit')
    
    args = parser.parse_args()
    
    if args.create_config:
        create_example_config()
        return

    config_data = None
    try:
        with open(args.config, 'r') as f:
            config_data = yaml.safe_load(f)
        if not isinstance(config_data, dict):
            logger.error(f"Error: Config file {args.config} is not a valid YAML dictionary. Exiting.")
            return
        if not config_data:
            logger.error(f"Error: Config file {args.config} is empty or invalid. Exiting.")
            return
            
    except FileNotFoundError:
        logger.error(f"Error: Config file '{args.config}' not found. Use --create-config to generate an example. Exiting.")
        return
    except Exception as e:
        logger.error(f"Error reading or parsing config file {args.config}: {e}. Exiting.")
        return
    
    # Initialize PriceAlert with the loaded config_data
    monitor = PriceAlert(config_data=config_data, main_loop_interval_seconds=args.main_loop_interval)
    
    if args.test_email:
        monitor.send_test_alert()
        return
    
    monitor.run()

if __name__ == "__main__":
    main() 
--- End File: {relative_path} ---

--- File: collectors\__init__.py ---
# sentinel/collectors/__init__.py 
--- End File: {relative_path} ---

--- File: collectors\coinbase.py ---
# sentinel/collectors/coinbase.py
import asyncio
import logging
from typing import Callable # For type hinting the sink

from trade_suite.data.data_source import Data # Assuming Data class is accessible
from sentinel import schema # For LP building
from sentinel import config # For constants like CADENCE_MS

async def stream_data_to_queues(
    data_source: Data,
    symbol: str,
    stop_event: asyncio.Event,
    trade_queue: asyncio.Queue,
    order_book_queue: asyncio.Queue,
    is_raw_enabled: bool, # New: To control raw book processing
    raw_order_book_queue: asyncio.Queue | None = None, # New: Queue for raw order book LP
    exchange_name: str = config.TARGET_EXCHANGE,
    order_book_cadence_ms: int = config.CADENCE_MS,
    max_queue_retries: int = 3, # Max retries for putting on queue if full
    queue_retry_delay: float = 0.01 # Delay between retries in seconds
):
    """
    Uses the Data class to watch trades and order books, 
    formats them into Line Protocol, and puts them onto asyncio Queues.
    Includes queue overflow checks and optional raw order book processing.

    Args:
        data_source: An initialized instance of the Data class.
        symbol: The trading symbol (e.g., 'BTC/USD').
        stop_event: asyncio.Event to signal when to stop streaming.
        trade_queue: asyncio.Queue to send trade Line Protocol strings to.
        order_book_queue: asyncio.Queue to send binned order book Line Protocol strings to.
        is_raw_enabled: Boolean indicating if raw order book data should be processed.
        raw_order_book_queue: Optional asyncio.Queue for raw order book LP strings.
        exchange_name: The name of the exchange to stream from.
        order_book_cadence_ms: The cadence for order book updates in milliseconds.
        max_queue_retries: How many times to retry putting on a full queue.
        queue_retry_delay: Delay between queue put retries.
    """
    logger = logging.getLogger(__name__) # Get a logger specific to this module/function
    dropped_trades_count = 0
    dropped_binned_books_count = 0
    dropped_raw_books_count = 0

    if is_raw_enabled and raw_order_book_queue is None:
        logger.warning(f"[{exchange_name.upper()}] Raw order book is enabled, but no raw_order_book_queue provided. Raw data will not be processed.")
        is_raw_enabled = False # Disable if queue is missing

    logger.info(f"[{exchange_name.upper()}] Starting data collection for {symbol}. Raw enabled: {is_raw_enabled}")

    async def _safe_put_to_queue(q: asyncio.Queue, item: any, item_type: str) -> bool:
        nonlocal dropped_trades_count, dropped_binned_books_count, dropped_raw_books_count
        for attempt in range(max_queue_retries):
            if q.full():
                logger.warning(f"Queue for {item_type} is full (size: {q.qsize()}). Attempt {attempt + 1}/{max_queue_retries}. Retrying after {queue_retry_delay}s...")
                if attempt == max_queue_retries - 1: # Last attempt failed
                    logger.critical(f"CRITICAL: Queue for {item_type} remained full after {max_queue_retries} attempts. Dropping data for {symbol} on {exchange_name}.")
                    if item_type == 'trade': dropped_trades_count += 1
                    elif item_type == 'binned_book': dropped_binned_books_count += 1
                    elif item_type == 'raw_book': dropped_raw_books_count += 1
                    # TODO: Expose these counts to healthz or metrics
                    return False # Failed to put
                await asyncio.sleep(queue_retry_delay)
            else:
                await q.put(item)
                return True # Successfully put
        return False # Should be unreachable if loop logic is correct

    async def trade_sink(trade_event_dict):
        try:
            exchange = trade_event_dict['exchange']
            trade = trade_event_dict['trade_data']
            
            # Extract sequence if available (though typically not in simple trade data from watch_trades)
            # sequence = trade.get('info', {}).get('sequence') # Example path, adjust as needed
            
            timestamp_ns = int(trade['timestamp']) * 1_000_000
            lp = schema.build_trade_lp(
                exchange=exchange,
                symbol=trade['symbol'],
                side=trade['side'],
                size=trade['amount'],
                price=trade['price'],
                trade_id=str(trade['id']),
                timestamp_ns=timestamp_ns
            )
            logger.debug(f"[{exchange_name.upper()}] Trade LP generated: {lp}")
            await _safe_put_to_queue(trade_queue, lp, 'trade')
        except Exception as e:
            logger.error(f"[{exchange_name.upper()}] Error processing trade data for sink: {e} - Data: {trade_event_dict}", exc_info=True)

    async def order_book_sink(order_book_event_dict):
        try:
            exchange = order_book_event_dict['exchange']
            book = order_book_event_dict['orderbook']
            timestamp_ms = book['timestamp']
            timestamp_ns = int(timestamp_ms) * 1_000_000
            
            # Attempt to get sequence number (highly exchange-specific for full books from watchOrderBook)
            # For Coinbase, `nonce` is often the sequence for snapshots, or info.sequence for L2 updates.
            # This needs verification for what `watch_order_book` provides from `trade_suite.data_source`
            sequence = book.get('nonce') # Common for CCXT snapshots
            if sequence is None and 'info' in book and isinstance(book['info'], dict):
                sequence = book['info'].get('sequence') # Try info.sequence (e.g. Coinbase Pro REST snapshot)
            
            # Binned order book processing
            binned_lp_lines = schema.build_book_lp(
                exchange=exchange,
                symbol=book['symbol'],
                bids=book['bids'],
                asks=book['asks'],
                timestamp_ns=timestamp_ns,
                sequence=sequence
            )
            if binned_lp_lines:
                await _safe_put_to_queue(order_book_queue, binned_lp_lines, 'binned_book')

            # Raw order book processing (if enabled)
            if is_raw_enabled and raw_order_book_queue:
                raw_lp_lines = schema.build_raw_book_lp(
                    exchange=exchange,
                    symbol=book['symbol'],
                    bids=book['bids'],
                    asks=book['asks'],
                    timestamp_ns=timestamp_ns,
                    top_n=config.RAW_BOOK_TOP_N,
                    sequence=sequence
                )
                if raw_lp_lines:
                    logger.debug(f"[{exchange_name.upper()}] Raw LP lines generated for {book['symbol']}: {raw_lp_lines}")
                    await _safe_put_to_queue(raw_order_book_queue, raw_lp_lines, 'raw_book')
                    
        except Exception as e:
            logger.error(f"[{exchange_name.upper()}] Error processing order book data for sink: {e} - Data: {order_book_event_dict}", exc_info=True)

    # Create tasks for watching trades and order books
    # NOTE: data_source.watch_trades and data_source.watch_orderbook need to be updated
    # to accept `sink` and `cadence_ms` parameters respectively.
    trade_watcher_task = asyncio.create_task(
        data_source.watch_trades(
            symbol=symbol,
            exchange=exchange_name, 
            stop_event=stop_event,
            track_stats=False, # Sentinel does not need stats from data_source
            write_trades=False, # Sentinel handles its own writing
            write_stats=False, # Sentinel handles its own writing
            sink=trade_sink
        )
    )
    # Pass the specific cadence for order books to watch_orderbook
    order_book_watcher_task = asyncio.create_task(
        data_source.watch_orderbook(
            exchange=exchange_name,
            symbol=symbol,
            stop_event=stop_event,
            sink=order_book_sink,
            cadence_ms=order_book_cadence_ms
        )
    )

    logger.info(f"[{exchange_name.upper()}] Trade and order book watchers for {symbol} started.")

    try:
        # Wait for tasks to complete or stop_event to be set
        # This can be managed by the supervisor which calls this function
        await asyncio.gather(trade_watcher_task, order_book_watcher_task)
    except asyncio.CancelledError:
        logger.info(f"[{exchange_name.upper()}] Data collection for {symbol} cancelled.")
    finally:
        if not trade_watcher_task.done():
            trade_watcher_task.cancel()
        if not order_book_watcher_task.done():
            order_book_watcher_task.cancel()
        logger.info(f"[{exchange_name.upper()}] Data collection for {symbol} stopped.")

# Placeholder for Binance or other exchange collectors
# async def stream_btc_binance(queue: asyncio.Queue):
#     pass 
--- End File: {relative_path} ---

--- File: config.py ---
# sentinel/config.py

# Websocket and data processing configuration
CADENCE_MS = 100  # Target time in milliseconds for order book snapshots (10 Hz)
DEPTH_BINS = 5  # Number of price bins on each side (bids/asks) of the mid-price
SNAPSHOT_POINTS = 10  # Total data points for an order book snapshot (2 * DEPTH_BINS typically)

# Order book binning configuration (New)
ORDER_BOOK_BIN_BPS = 5          # Basis points for each bin width (e.g., 5 bps = 0.05%)
ORDER_BOOK_MAX_BINS_PER_SIDE = 5 # Number of bins to generate on each side of the mid-price (e.g., 5 bins for bids, 5 for asks)

# Raw Order Book configuration (New)
RAW_BOOK_TOP_N = 10             # Number of top bid/ask levels for raw order book data
INFLUX_BUCKET_OB_RAW = "raw_order_book" # Bucket for raw order book data

# InfluxDB configuration
INFLUX_BUCKET_OB = "order_book"       # Bucket for order book data
INFLUX_BUCKET_TR = "trades"           # Bucket for trade data
INFLUX_ORG = "pepe"                   # InfluxDB organization (as per your existing InfluxDB class)
INFLUX_URL_LOCAL = "http://localhost:8086"
# INFLUX_URL_CLOUD = "https://us-east-1-1.aws.cloud2.influxdata.com" # Example, if you use cloud
# INFLUX_TOKEN_ENV_VAR_LOCAL = "INFLUXDB_TOKEN_LOCAL" # Environment variable for local token
# INFLUX_TOKEN_ENV_VAR_CLOUD = "INFLUXDB"           # Environment variable for cloud token

# Collector/Writer behavior
WS_RECONNECT_BACKOFF = [1, 2, 5, 10]  # Seconds to wait before WebSocket reconnection attempts
WRITER_BATCH_SIZE_POINTS = 5000       # Max points to batch before writing to InfluxDB
WRITER_FLUSH_INTERVAL_MS = 100      # Max time to wait before flushing batch to InfluxDB

# Logging configuration
LOG_FILE = "./sentinel.log"         # Path to the log file
LOG_LEVEL = "INFO"                  # Default logging level (e.g., DEBUG, INFO, WARNING, ERROR)

# Run configuration
RUN_DURATION_SECONDS_DRY_RUN = 300 # 5 minutes, was 30 seconds
RUN_DURATION_SECONDS_LIVE = 48 * 60 * 60  # 48 hours
RUN_DURATION_SECONDS_TEST = 1 * 60    # 1 minute for integration tests

# Health Check Interval
# How often the supervisor should log the status of its queues (in seconds)
HEALTH_CHECK_INTERVAL_SECONDS = 30

# Targetted assets and exchanges
TARGET_EXCHANGE = "coinbase"
TARGET_SYMBOL_CCXT = "BTC/USD" # CCXT format
TARGET_SYMBOL_INFLUX = "BTC-USD" # Format for InfluxDB tags/fields if different 
--- End File: {relative_path} ---

--- File: quick_start.py ---
import asyncio

async def smoke():
    from trade_suite.data.data_source import Data
    import asyncio, os
    ds = Data(influx=None, emitter=None, exchanges=['coinbase'], force_public=True)
    await ds.load_exchanges()
    stop = asyncio.Event()
    async def dummy(event): pass
    t1 = asyncio.create_task(ds.watch_orderbook('coinbase','BTC/USD', stop, sink=dummy, cadence_ms=200))
    await asyncio.sleep(3)
    stop.set(); await t1
    
asyncio.run(smoke())

--- End File: {relative_path} ---

--- File: run.py ---
# sentinel/run.py
import asyncio
import sys

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

import argparse
import logging
import signal
import os

from sentinel.supervisor import Supervisor
from sentinel import config

# Logger setup is handled in supervisor.py, but run.py can also use it.
logger = logging.getLogger("sentinel.run")

async def main(args):
    """Main function to initialize and run the Supervisor."""
    supervisor = None # Initialize to None for finally block
    try:
        supervisor = Supervisor(is_raw_enabled=args.raw)
    except ValueError as e:
        logger.critical(f"Failed to initialize Supervisor: {e}. Ensure INFLUXDB_TOKEN_LOCAL is set.")
        return # Exit if supervisor cannot be initialized
    
    # Handle graceful shutdown on SIGINT and SIGTERM
    loop = asyncio.get_event_loop()
    
    def signal_handler():
        logger.info("Signal received, initiating graceful shutdown...")
        asyncio.create_task(supervisor.stop()) # Schedule stop without blocking handler

    for sig in (signal.SIGINT, signal.SIGTERM):
        try:
            loop.add_signal_handler(sig, signal_handler)
        except NotImplementedError:
            # Windows does not support add_signal_handler for SIGINT/SIGTERM in the same way
            # For Windows, KeyboardInterrupt is the primary way to stop for SIGINT.
            # SIGTERM might need other platform-specific handling if strictly required.
            logger.warning(f"Signal handler for {sig.name} could not be set (likely on Windows).")
            pass

    duration = None
    if args.dry_run:
        logger.info("Executing DRAGON dry run... I mean DRY RUN.")
        duration = config.RUN_DURATION_SECONDS_DRY_RUN
        # In a true dry run, we might also redirect InfluxWriter to a mock or stdout.
        # For now, it will run for a short duration.
    elif args.live:
        logger.info("Executing LIVE run... Strap in!")
        duration = config.RUN_DURATION_SECONDS_LIVE # 48 hours as per plan
    else:
        # Default to a short dry run if no mode is specified
        logger.info("No run mode specified, defaulting to a short dry run.")
        duration = config.RUN_DURATION_SECONDS_DRY_RUN

    try:
        await supervisor.start(duration_seconds=duration)
    except KeyboardInterrupt:
        logger.info("Keyboard interrupt received by run.py. Supervisor should handle shutdown.")
        # Supervisor's signal handler or its own KeyboardInterrupt handling should manage this.
        # If supervisor.start() is still running, it will eventually call supervisor.stop()
        # or the signal handler will.
    except Exception as e:
        logger.exception(f"Unhandled exception in run.py main loop: {e}")
    finally:
        logger.info("Run.py main function cleanup starting...")
        if supervisor and not supervisor.stop_event.is_set():
            logger.info("Ensuring supervisor is stopped in finally block.")
            # This call is a safeguard. Ideally, supervisor.start or signal handler manages stop.
            await supervisor.stop() 
        logger.info("Run.py main function finished.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Sentinel: BTC Order Book and Trade Recorder.")
    group = parser.add_mutually_exclusive_group() # Allow either --dry-run or --live, but not both
    group.add_argument("--dry-run", action="store_true", help="Run for a short duration (30s) and print logs.")
    group.add_argument("--live", action="store_true", help="Run continuously for the configured duration (e.g., 48h).")
    parser.add_argument("--raw", action="store_true", help="Enable collection and writing of raw top-N order book data to a separate bucket.")

    args = parser.parse_args()

    # Ensure INFLUXDB_TOKEN_LOCAL is available before trying to run
    if not os.getenv("INFLUXDB_TOKEN_LOCAL"):
        print("CRITICAL: INFLUXDB_TOKEN_LOCAL environment variable is not set. Sentinel cannot run.")
        print("Please set this environment variable with your local InfluxDB token.")
    else:
        try:
            asyncio.run(main(args))
        except KeyboardInterrupt:
            logger.info("Application terminated by KeyboardInterrupt at the top level.")
        except Exception as e:
            logger.critical(f"Top-level unhandled exception: {e}", exc_info=True) 
--- End File: {relative_path} ---

--- File: schema.py ---
# sentinel/schema.py
from typing import List, Tuple, Dict

# Expected trade data structure from data_source.watch_trades
# trade_event = {
#     'exchange': str, # e.g., 'coinbase'
#     'trade_data': {
#         'id': str, # Trade ID
#         'timestamp': int, # Milliseconds since epoch
#         'datetime': str, # ISO8601 datetime string
#         'symbol': str, # e.g., 'BTC/USD'
#         'side': str, # 'buy' or 'sell'
#         'price': float,
#         'amount': float,
#         'cost': float, # price * amount
#         # 'takerOrMaker': str, (optional)
#         # 'fee': { 'cost': float, 'currency': str }, (optional)
#         # 'info': dict (original exchange response)
#     }
# }

def build_trade_lp(exchange: str, symbol: str, side: str, size: float, price: float, trade_id: str, timestamp_ns: int) -> str:
    """
    Builds a InfluxDB Line Protocol string for a single trade.

    Args:
        exchange: Name of the exchange (e.g., 'coinbase').
        symbol: Trading symbol (e.g., 'BTC-USD', sanitized for InfluxDB).
        side: 'buy' or 'sell'.
        size: Quantity of the trade.
        price: Price of the trade.
        trade_id: Unique identifier for the trade from the exchange.
        timestamp_ns: Timestamp of the trade in nanoseconds.

    Returns:
        A string formatted for InfluxDB Line Protocol.
        Example: trades,exchange=coinbase,symbol=BTC-USD,side=buy trade_id="12345",price=50000.1,size=0.01 1678886400000000000
    """
    # Sanitize symbol: CCXT uses 'BTC/USD', InfluxDB prefers 'BTC-USD' or similar for tags
    safe_symbol = symbol.replace("/", "-")
    # Fields must be float, int, bool, or string. Strings need to be double-quoted.
    # Tags are not quoted.
    lp = f"trades,exchange={exchange},symbol={safe_symbol},side={side} " \
         f"trade_id=\"{trade_id}\",price={float(price)},size={float(size)} " \
         f"{timestamp_ns}"
    return lp


# Expected order book data structure from data_source.watch_orderbook
# orderbook_event = {
#     'exchange': str, # e.g., 'coinbase'
#     'orderbook': {
#         'symbol': str, # e.g., 'BTC/USD'
#         'timestamp': int, # Milliseconds since epoch for the snapshot
#         'datetime': str, # ISO8601 string
#         'bids': List[Tuple[float, float]], # List of [price, amount]
#         'asks': List[Tuple[float, float]], # List of [price, amount]
#         # 'nonce': int (optional)
#         # 'info': { 'sequence': int } (for Coinbase Pro, sequence is often in info for snapshots from REST)
#     }
# }

from sentinel import config # For binning constants
import math # For rounding

def build_book_lp(
    exchange: str, 
    symbol: str, 
    bids: List[Tuple[float, float]], 
    asks: List[Tuple[float, float]], 
    timestamp_ns: int,
    sequence: int | None = None # Optional: sequence number for the book snapshot
) -> List[str]:
    """
    Builds InfluxDB Line Protocol strings for an order book snapshot, binned by basis points (bps) from mid-price.
    Generates a fixed number of points based on config.ORDER_BOOK_MAX_BINS_PER_SIDE.

    Args:
        exchange: Name of the exchange.
        symbol: Trading symbol (e.g., 'BTC-USD').
        bids: Raw list of [price, amount] for bids, sorted best (highest price) first.
        asks: Raw list of [price, amount] for asks, sorted best (lowest price) first.
        timestamp_ns: Timestamp of the snapshot in nanoseconds.
        sequence: Optional sequence number of this order book state.

    Returns:
        A list of strings formatted for InfluxDB Line Protocol for binned book data.
        Example: order_book,exchange=cb,symbol=BTC-USD,side=bid,bin_bps_offset=-1 total_qty=1.23 1678886400000000000
    """
    lines = []
    safe_symbol = symbol.replace("/", "-")

    if not bids or not asks or not bids[0] or not asks[0]:
        # logging.warning(f"[{exchange}-{safe_symbol}] Insufficient data for binned order book: bids or asks empty or invalid.")
        return [] # Cannot calculate mid-price or meaningful book

    mid_price = (bids[0][0] + asks[0][0]) / 2.0
    if mid_price == 0: # Avoid division by zero
        # logging.warning(f"[{exchange}-{safe_symbol}] Mid price is zero, cannot bin order book.")
        return []

    # Initialize bins: keys are bps offsets from -MAX_BINS to +MAX_BINS (relative to BIN_BPS)
    # e.g., if MAX_BINS_PER_SIDE = 5, bins from -5 to 5.
    # Bin 0 represents quotes closest to mid-price within +/- (BIN_BPS/2).
    num_bins_total = config.ORDER_BOOK_MAX_BINS_PER_SIDE * 2 + 1 # e.g., 5 for bids, 5 for asks, 1 for zero-offset
    # bins will store total quantity for each bps_offset bin
    # We will create MAX_BINS_PER_SIDE on bid side (negative offsets) and MAX_BINS_PER_SIDE on ask side (positive offsets)
    # Bin 0 represents the very center (e.g. -2.5bps to +2.5bps if BIN_BPS is 5)
    # Let's use MAX_BINS_PER_SIDE for each side, so -5 to -1 for bids, 1 to 5 for asks, bin 0 for center.
    
    # bins dict: key is integer bps_offset_index, value is aggregated quantity.
    # bps_offset_index ranges from -config.ORDER_BOOK_MAX_BINS_PER_SIDE to +config.ORDER_BOOK_MAX_BINS_PER_SIDE.
    binned_quantities = {i: 0.0 for i in range(-config.ORDER_BOOK_MAX_BINS_PER_SIDE, config.ORDER_BOOK_MAX_BINS_PER_SIDE + 1)}

    # Process bids
    for price, qty in bids:
        if price <= 0: continue
        # bps_offset = (price - mid_price) / mid_price * 10000 # Raw BPS offset
        # bps_offset_index = math.floor(bps_offset / config.ORDER_BOOK_BIN_BPS) # Gist example used round
        # Let's follow the gist's rounding logic: int(round((price - mid) / mid * 1e4 / BIN_BPS))
        bps_offset_index = int(round((price - mid_price) / mid_price * 10000 / config.ORDER_BOOK_BIN_BPS))
        
        # Clamp to the defined bin range
        if bps_offset_index < -config.ORDER_BOOK_MAX_BINS_PER_SIDE:
            bps_offset_index = -config.ORDER_BOOK_MAX_BINS_PER_SIDE
        elif bps_offset_index > config.ORDER_BOOK_MAX_BINS_PER_SIDE: # This case mainly for asks, but good to be robust
            bps_offset_index = config.ORDER_BOOK_MAX_BINS_PER_SIDE
        
        if -config.ORDER_BOOK_MAX_BINS_PER_SIDE <= bps_offset_index <= config.ORDER_BOOK_MAX_BINS_PER_SIDE:
            binned_quantities[bps_offset_index] += qty

    # Process asks
    for price, qty in asks:
        if price <= 0: continue
        bps_offset_index = int(round((price - mid_price) / mid_price * 10000 / config.ORDER_BOOK_BIN_BPS))
        
        if bps_offset_index < -config.ORDER_BOOK_MAX_BINS_PER_SIDE:
            bps_offset_index = -config.ORDER_BOOK_MAX_BINS_PER_SIDE
        elif bps_offset_index > config.ORDER_BOOK_MAX_BINS_PER_SIDE:
            bps_offset_index = config.ORDER_BOOK_MAX_BINS_PER_SIDE

        if -config.ORDER_BOOK_MAX_BINS_PER_SIDE <= bps_offset_index <= config.ORDER_BOOK_MAX_BINS_PER_SIDE:
            binned_quantities[bps_offset_index] += qty
    
    # Generate Line Protocol for each bin that has quantity or all defined bins
    # The gist implies emitting all defined bins, even if quantity is 0, for constant cardinality.
    for bps_offset_idx, total_qty in binned_quantities.items():
        # Determine side based on offset. Bin 0 can be considered neutral or assigned based on convention.
        # For simplicity, let's say negative is bid, positive is ask. Bin 0 can be split or reported as 'mid'.
        # The gist example: side = "bid" if off_bp < 0 else "ask"
        # This means bin 0 is 'ask'. If that's not desired, adjust.
        # Let's adjust so bin 0 is 'mid', <0 is 'bid', >0 is 'ask' for clarity in tags.
        if bps_offset_idx < 0:
            side = "bid"
        elif bps_offset_idx > 0:
            side = "ask"
        else: # bps_offset_idx == 0
            side = "mid_bin" # A neutral bin at the center
        
        # Measurement: order_book_binned (or just order_book if only one type of book data per bucket)
        lp = f"order_book,exchange={exchange},symbol={safe_symbol},side={side},bps_offset_idx={bps_offset_idx} " \
             f"total_qty={total_qty:.8f},mid_price={mid_price:.2f}" # Added mid_price, formatted to 2 decimal places
        if sequence is not None:
            lp += f",sequence={sequence}i" # Add sequence as an integer field
        lp += f" {timestamp_ns}"
        lines.append(lp)

    return lines


def build_raw_book_lp(
    exchange: str, 
    symbol: str, 
    bids: List[Tuple[float, float]], 
    asks: List[Tuple[float, float]], 
    timestamp_ns: int,
    top_n: int = config.RAW_BOOK_TOP_N, # Use constant from config
    sequence: int | None = None # Optional: sequence number
) -> List[str]:
    """
    Builds InfluxDB Line Protocol strings for the top N raw levels of an order book snapshot.

    Args:
        exchange: Name of the exchange.
        symbol: Trading symbol (e.g., 'BTC-USD').
        bids: Raw list of [price, amount] for bids, sorted best (highest price) first.
        asks: Raw list of [price, amount] for asks, sorted best (lowest price) first.
        timestamp_ns: Timestamp of the snapshot in nanoseconds.
        top_n: Number of top bid and ask levels to include.
        sequence: Optional sequence number of this order book state.

    Returns:
        A list of strings formatted for InfluxDB Line Protocol for raw book data.
        Example: raw_order_book,exchange=cb,symbol=BTC-USD,side=bid,level=0 price=49999.0,amount=0.5 1678886400000000000
    """
    lines = []
    safe_symbol = symbol.replace("/", "-")

    # Process top N bids
    for i, (price, amount) in enumerate(bids[:top_n]):
        lp = f"raw_order_book,exchange={exchange},symbol={safe_symbol},side=bid,level={i} " \
             f"price={float(price)},amount={float(amount):.8f}" # Format amount
        if sequence is not None:
            lp += f",sequence={sequence}i"
        lp += f" {timestamp_ns}"
        lines.append(lp)

    # Process top N asks
    for i, (price, amount) in enumerate(asks[:top_n]):
        lp = f"raw_order_book,exchange={exchange},symbol={safe_symbol},side=ask,level={i} " \
             f"price={float(price)},amount={float(amount):.8f}" # Format amount
        if sequence is not None:
            lp += f",sequence={sequence}i"
        lp += f" {timestamp_ns}"
        lines.append(lp)
        
    return lines 
--- End File: {relative_path} ---

--- File: supervisor.py ---
# sentinel/supervisor.py
import asyncio
import logging
import os
import signal # For graceful shutdown
import logging.handlers # For RotatingFileHandler

from trade_suite.data.data_source import Data as TradeSuiteData # Alias to avoid confusion
from sentinel.collectors.coinbase import stream_data_to_queues
from sentinel.writers.influx_writer import InfluxWriter
from sentinel import config

# Basic logging setup - consider using structlog as planned for richer logs
# Get the root logger
root_logger = logging.getLogger() 
root_logger.setLevel(getattr(logging, config.LOG_LEVEL.upper(), logging.INFO))

# Quieten noisy libraries if root is DEBUG
if root_logger.level == logging.DEBUG:
    for lib_name in ["ccxt", "aiohttp", "websockets"]:
        logging.getLogger(lib_name).setLevel(logging.INFO)

# File Handler with Rotation
file_handler = logging.handlers.RotatingFileHandler(
    config.LOG_FILE, 
    maxBytes=10*1024*1024, # e.g., 10 MB per file
    backupCount=5          # Keep 5 backup files
)
file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)
root_logger.addHandler(file_handler)

# Console Handler
console_handler = logging.StreamHandler()
console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s') # Simpler format for console
console_handler.setFormatter(console_formatter)
root_logger.addHandler(console_handler)

logger = logging.getLogger(__name__) # Supervisor specific logger

class Supervisor:
    def __init__(self, is_raw_enabled: bool = False): # Accept is_raw_enabled
        self.is_raw_enabled = is_raw_enabled
        self.trade_queue = asyncio.Queue(maxsize=10000)
        self.order_book_queue = asyncio.Queue(maxsize=10000)
        self.raw_order_book_queue = asyncio.Queue(maxsize=10000) if self.is_raw_enabled else None
        self.stop_event = asyncio.Event()
        self.tasks = []

        # For healthz
        self.dropped_trades_count = 0
        self.dropped_binned_books_count = 0
        self.dropped_raw_books_count = 0
        # The collector will have its own counters, this is illustrative
        # The actual counters should be accessed from collector or passed via a shared mechanism if needed here

        # Initialize InfluxWriter
        influx_token = os.getenv("INFLUXDB_TOKEN_LOCAL") # Or your preferred token env var
        if not influx_token:
            logger.critical("INFLUXDB_TOKEN_LOCAL environment variable not set. Sentinel cannot start.")
            raise ValueError("InfluxDB token not found in environment.")
        
        self.influx_writer = InfluxWriter(
            influx_url=config.INFLUX_URL_LOCAL,
            influx_token=influx_token,
            influx_org=config.INFLUX_ORG
        )

        # Initialize TradeSuiteData (DataSource)
        # No emitter needed for sentinel's use case. Influx client is managed by InfluxWriter.
        self.data_source = TradeSuiteData(influx=None, emitter=None, exchanges=[config.TARGET_EXCHANGE], force_public=True)

    async def _run_with_restart(self, coro_func, *args, name="UnnamedTask"):
        """Runs a coroutine and restarts it with exponential backoff on failure."""
        backoff_times = config.WS_RECONNECT_BACKOFF
        attempt = 0
        while not self.stop_event.is_set():
            try:
                logger.info(f"Starting task: {name}")
                await coro_func(*args)
                # If the coro_func returns normally, it might mean it completed (e.g. stop_event was set internally)
                # or an unexpected exit. If it's not due to stop_event, we might want to restart.
                if self.stop_event.is_set():
                    logger.info(f"Task {name} stopped gracefully via stop_event.")
                    break
                else:
                    logger.warning(f"Task {name} exited unexpectedly. Restarting after delay...")
                    # This case might need specific handling based on why a task would exit normally
                    # without stop_event being set.
            except asyncio.CancelledError:
                logger.info(f"Task {name} was cancelled.")
                break # Do not restart if explicitly cancelled
            except Exception as e:
                logger.error(f"Task {name} failed with error: {e}. Attempt {attempt + 1}.")
            
            if self.stop_event.is_set():
                break

            if attempt < len(backoff_times):
                delay = backoff_times[attempt]
                logger.info(f"Restarting {name} in {delay} seconds...")
                await asyncio.sleep(delay)
                attempt += 1
            else:
                logger.error(f"Task {name} failed maximum restart attempts. Giving up.")
                self.stop_event.set() # Signal other tasks to stop as a critical component failed
                break
        logger.info(f"Task {name} has finished its lifecycle.")

    async def _healthz(self, interval_seconds: int = 30):
        """Periodically logs queue sizes and other health metrics."""
        logger.info("Healthz monitor started.")
        while not self.stop_event.is_set():
            try:
                # Note: Accessing internal collector counters directly isn't clean.
                # Ideally, collector exposes these or healthz is part of the collector, 
                # or metrics are pushed to a central place (like Prometheus later).
                # For now, logging queue sizes is a good start.
                trade_q_size = self.trade_queue.qsize()
                binned_book_q_size = self.order_book_queue.qsize()
                raw_book_q_size = self.raw_order_book_queue.qsize() if self.raw_order_book_queue else 'N/A'
                
                logger.info(
                    f"[Healthz] Queues - Trades: {trade_q_size}, BinnedBooks: {binned_book_q_size}, RawBooks: {raw_book_q_size}"
                    # f", Dropped - Trades: {self.dropped_trades_count}, Binned: {self.dropped_binned_books_count}, Raw: {self.dropped_raw_books_count}"
                )
                # Resetting supervisor-level counters would require collector to update them.
                # For simplicity, we'll rely on collector logs for drops for now.
                
                await asyncio.sleep(interval_seconds)
            except asyncio.CancelledError:
                logger.info("Healthz monitor cancelled.")
                break
            except Exception as e:
                logger.error(f"Healthz monitor encountered an error: {e}", exc_info=True)
                # Avoid healthz crashing the supervisor; wait and continue
                await asyncio.sleep(interval_seconds) 
        logger.info("Healthz monitor stopped.")

    async def start(self, duration_seconds: float | None = None):
        """Starts the collector and writer tasks and manages them."""
        logger.info("Supervisor starting...")
        if not self.influx_writer or not self.influx_writer.write_api:
            logger.critical("InfluxWriter not properly initialized. Supervisor cannot start data flow.")
            return

        await self.data_source.load_exchanges() # Important: Load exchange details
        if config.TARGET_EXCHANGE not in self.data_source.exchange_list:
            logger.critical(f"Target exchange '{config.TARGET_EXCHANGE}' not loaded in DataSource. Aborting.")
            return

        # Collector task
        collector_task = asyncio.create_task(
            self._run_with_restart(
                stream_data_to_queues,
                self.data_source,
                config.TARGET_SYMBOL_CCXT,
                self.stop_event,
                self.trade_queue,
                self.order_book_queue,
                self.is_raw_enabled, # Pass is_raw_enabled
                self.raw_order_book_queue, # Pass raw queue
                config.TARGET_EXCHANGE,
                config.CADENCE_MS,
                name="DataCollector"
            )
        )
        self.tasks.append(collector_task)

        # Writer task for trades
        trade_writer_task = asyncio.create_task(
            self._run_with_restart(
                self.influx_writer.run_queue_consumer,
                self.trade_queue,
                config.INFLUX_BUCKET_TR,
                self.stop_event,
                name="TradeInfluxWriter"
            )
        )
        self.tasks.append(trade_writer_task)

        # Writer task for order books
        ob_writer_task = asyncio.create_task(
            self._run_with_restart(
                self.influx_writer.run_queue_consumer,
                self.order_book_queue,
                config.INFLUX_BUCKET_OB,
                self.stop_event,
                name="OrderBookInfluxWriter"
            )
        )
        self.tasks.append(ob_writer_task)

        # Writer task for raw order books (if enabled)
        if self.is_raw_enabled and self.raw_order_book_queue:
            raw_ob_writer_task = asyncio.create_task(
                self._run_with_restart(
                    self.influx_writer.run_queue_consumer,
                    self.raw_order_book_queue,
                    config.INFLUX_BUCKET_OB_RAW, # Use the new raw bucket config
                    self.stop_event,
                    name="RawOrderBookInfluxWriter"
                )
            )
            self.tasks.append(raw_ob_writer_task)
            logger.info("Raw order book writer task created.")

        # Healthz task
        healthz_task = asyncio.create_task(self._healthz())
        self.tasks.append(healthz_task)

        logger.info("All tasks created. Supervisor is running.")

        if duration_seconds:
            logger.info(f"Running for a specified duration: {duration_seconds} seconds.")
            await asyncio.sleep(duration_seconds)
            logger.info("Specified duration ended. Initiating shutdown.")
            await self.stop()
        else:
            # Keep running until stop_event is set (e.g., by signal handler or error)
            await self.stop_event.wait()
            logger.info("Stop event received. Initiating shutdown.")
            # Ensure stop is called if loop exited due to stop_event
            # This path is taken if stop() is called from elsewhere, like a signal handler

        # Fallback: if duration_seconds was None and stop_event was set, ensure cleanup
        # await self.stop() # This might be redundant if stop() is what set the event

    async def stop(self):
        logger.info("Supervisor stopping... Setting stop event.")
        self.stop_event.set()

        # Wait for tasks to complete with a timeout
        # Give some time for tasks to handle the stop_event and flush
        logger.info(f"Waiting for {len(self.tasks)} tasks to finish...")
        done, pending = await asyncio.wait(self.tasks, timeout=10.0) # 10s timeout for graceful shutdown

        for task in pending:
            logger.warning(f"Task {task.get_name()} did not finish in time. Cancelling...")
            task.cancel()
        
        # Re-await pending tasks to process cancellations
        if pending:
             await asyncio.wait(pending, timeout=5.0) 

        logger.info("Closing InfluxWriter...")
        if self.influx_writer:
            self.influx_writer.close()
        
        logger.info("Closing DataSource connections...")
        await self.data_source.close_all_exchanges() # Ensure data_source has this method or adapt

        logger.info("Supervisor stopped.")

# Main execution / CLI entry point would typically be in run.py
# This is just for structure
async def main_supervisor_test(duration=10, is_raw_enabled_test=False):
    s = Supervisor(is_raw_enabled=is_raw_enabled_test)
    try:
        await s.start(duration_seconds=duration)
    except ValueError as e:
        logger.critical(f"Supervisor initialization failed: {e}")
    except KeyboardInterrupt:
        logger.info("Keyboard interrupt received. Stopping supervisor...")
        await s.stop()
    except Exception as e:
        logger.exception(f"Unhandled exception in supervisor main: {e}")
        await s.stop() # Attempt graceful shutdown
    finally:
        # Ensure cleanup happens even if start() wasn't awaited (e.g. init error)
        if not s.stop_event.is_set(): 
            await s.stop() # Call stop if it wasn't called yet)

if __name__ == "__main__":
    # This is a basic test run
    # In a real scenario, run.py would handle argparse and call this.
    asyncio.run(main_supervisor_test(duration=20, is_raw_enabled_test=True)) # Run for 20s for testing with raw enabled 
--- End File: {relative_path} ---

--- File: tests\test_schema.py ---
import unittest
from sentinel import schema
from sentinel import config # For accessing constants like RAW_BOOK_TOP_N for tests

class TestSchemaBuilders(unittest.TestCase):

    def test_build_trade_lp_example(self):
        exchange = "coinbase"
        symbol = "BTC/USD" # CCXT format
        safe_symbol_expected = "BTC-USD"
        side = "buy"
        size = 0.1
        price = 50000.0
        trade_id = "trd123"
        timestamp_ns = 1678886400123456789

        lp = schema.build_trade_lp(exchange, symbol, side, size, price, trade_id, timestamp_ns)
        
        expected_tags = f"trades,exchange={exchange},symbol={safe_symbol_expected},side={side}"
        expected_fields = f"trade_id=\"{trade_id}\",price={float(price)},size={float(size)}"
        expected_timestamp = str(timestamp_ns)

        self.assertTrue(lp.startswith(expected_tags))
        self.assertIn(expected_fields, lp)
        self.assertTrue(lp.endswith(expected_timestamp))
        self.assertEqual(lp.count(' '), 2) # Tags, Fields, Timestamp

    def test_build_book_lp_binned_example(self):
        exchange = "testex"
        symbol = "ETH/USD"
        safe_symbol_expected = "ETH-USD"
        # Bids sorted high to low, Asks sorted low to high
        bids = [(2998.0, 0.5), (2997.0, 1.0)] 
        asks = [(3002.0, 0.3), (3003.0, 0.8)]
        timestamp_ns = 1678886500987654321
        sequence = 12345

        # Expected mid_price = (2998.0 + 3002.0) / 2.0 = 3000.0
        # With BIN_BPS = 5 (0.05%), one bin width is 3000 * 0.0005 = 1.5 USD
        # Max bins per side = 5

        lp_lines = schema.build_book_lp(exchange, symbol, bids, asks, timestamp_ns, sequence)

        # Expected number of lines: (MAX_BINS_PER_SIDE * 2) + 1 (for mid_bin)
        expected_line_count = config.ORDER_BOOK_MAX_BINS_PER_SIDE * 2 + 1
        self.assertEqual(len(lp_lines), expected_line_count)

        found_specific_bid_bin = False
        found_specific_ask_bin = False
        found_mid_bin = False

        for lp in lp_lines:
            self.assertIn(f"order_book,exchange={exchange},symbol={safe_symbol_expected}", lp)
            self.assertIn(f"sequence={sequence}i", lp) # Check sequence field
            self.assertTrue(lp.endswith(str(timestamp_ns)))

            # Example check for a bid at 2998.0: (2998 - 3000) / 3000 * 10000 / 5 = -0.66 / 5 = -1.33 -> round = -1
            if f"side=bid,bps_offset_idx=-1" in lp:
                self.assertIn(f"total_qty=0.5", lp) # Approx, need to sum if multiple fall in same bin
                found_specific_bid_bin = True
            # Example check for an ask at 3002.0: (3002 - 3000) / 3000 * 10000 / 5 = 0.66 / 5 = 1.33 -> round = 1
            if f"side=ask,bps_offset_idx=1" in lp:
                self.assertIn(f"total_qty=0.3", lp)
                found_specific_ask_bin = True
            if f"side=mid_bin,bps_offset_idx=0" in lp:
                # qty might be 0 or sum of items very close to mid depending on exact rounding and data
                found_mid_bin = True
        
        self.assertTrue(found_specific_bid_bin, "Specific bid bin not found or incorrect.")
        self.assertTrue(found_specific_ask_bin, "Specific ask bin not found or incorrect.")
        self.assertTrue(found_mid_bin, "Mid bin not found.")

    def test_build_book_lp_binned_empty_input(self):
        lp_lines = schema.build_book_lp("test", "SYM", [], [], 123)
        self.assertEqual(len(lp_lines), 0)
        lp_lines = schema.build_book_lp("test", "SYM", [(1,1)], [], 123)
        self.assertEqual(len(lp_lines), 0)

    def test_build_raw_book_lp_example(self):
        exchange = "rawex"
        symbol = "ADA/USDT"
        safe_symbol_expected = "ADA-USDT"
        bids = [(1.00, 100.0), (0.99, 50.0)] * (config.RAW_BOOK_TOP_N // 2) # Ensure enough data
        asks = [(1.01, 80.0), (1.02, 60.0)] * (config.RAW_BOOK_TOP_N // 2)
        timestamp_ns = 1678886600000000000
        sequence = 54321

        lp_lines = schema.build_raw_book_lp(exchange, symbol, bids, asks, timestamp_ns, top_n=config.RAW_BOOK_TOP_N, sequence=sequence)
        
        expected_line_count = config.RAW_BOOK_TOP_N * 2 # N bids + N asks
        self.assertEqual(len(lp_lines), expected_line_count)

        for i in range(config.RAW_BOOK_TOP_N):
            bid_lp = lp_lines[i]
            ask_lp = lp_lines[i + config.RAW_BOOK_TOP_N]

            self.assertTrue(bid_lp.startswith(f"raw_order_book,exchange={exchange},symbol={safe_symbol_expected},side=bid,level={i}"))
            self.assertIn(f"price={float(bids[i][0])},amount={bids[i][1]:.8f}", bid_lp)
            self.assertIn(f"sequence={sequence}i", bid_lp)
            self.assertTrue(bid_lp.endswith(str(timestamp_ns)))

            self.assertTrue(ask_lp.startswith(f"raw_order_book,exchange={exchange},symbol={safe_symbol_expected},side=ask,level={i}"))
            self.assertIn(f"price={float(asks[i][0])},amount={asks[i][1]:.8f}", ask_lp)
            self.assertIn(f"sequence={sequence}i", ask_lp)
            self.assertTrue(ask_lp.endswith(str(timestamp_ns)))

    def test_build_raw_book_lp_less_data_than_top_n(self):
        bids = [(1.0, 10.0)]
        asks = [(1.1, 11.0)]
        lp_lines = schema.build_raw_book_lp("test", "SYM", bids, asks, 123, top_n=5)
        self.assertEqual(len(lp_lines), 2) # 1 bid + 1 ask
        self.assertIn("level=0", lp_lines[0])
        self.assertIn("level=0", lp_lines[1])

if __name__ == '__main__':
    unittest.main() 
--- End File: {relative_path} ---

--- File: writers\__init__.py ---
# sentinel/writers/__init__.py 
--- End File: {relative_path} ---

--- File: writers\influx_writer.py ---
# sentinel/writers/influx_writer.py
import asyncio
import logging
import os
from typing import List, Union, Any # Union for queue item type, Any for InfluxDB client

from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.exceptions import InfluxDBError
from influxdb_client.client.write_api import ASYNCHRONOUS, WriteOptions

from sentinel import config

class InfluxWriter:
    def __init__(self, influx_url: str, influx_token: str, influx_org: str):
        """
        Initializes the InfluxWriter with connection details for InfluxDB.

        Args:
            influx_url: URL of the InfluxDB instance.
            influx_token: Authentication token for InfluxDB.
            influx_org: Organization name in InfluxDB.
        """
        self.influx_url = influx_url
        self.influx_token = influx_token
        self.influx_org = influx_org
        self.client: InfluxDBClient | None = None
        self.write_api = None
        self._connect()

    def _connect(self):
        """Establishes connection to InfluxDB and initializes the write_api."""
        try:
            self.client = InfluxDBClient(
                url=self.influx_url,
                token=self.influx_token,
                org=self.influx_org
            )
            # Configure WriteOptions for batching
            # Using batch_size from config, flush_interval from config
            # jitter_interval and retry_interval can be added for more robust retries by the client library
            write_options = WriteOptions(
                batch_size=config.WRITER_BATCH_SIZE_POINTS,
                flush_interval=config.WRITER_FLUSH_INTERVAL_MS,
                write_type=ASYNCHRONOUS
            )
            self.write_api = self.client.write_api(write_options=write_options)
            logging.info("InfluxDB client initialized and write_api configured.")
            # Verify connection (optional, but good for early feedback)
            if not self.client.ping():
                 logging.warning("InfluxDB ping failed. Check connection and credentials.")
        except Exception as e:
            logging.error(f"Failed to connect to InfluxDB or initialize write_api: {e}")
            self.client = None # Ensure client is None if connection fails
            self.write_api = None

    async def write_batch(self, bucket: str, data_points: List[str]):
        """
        Writes a batch of Line Protocol data points to the specified InfluxDB bucket.
        Includes basic retry logic.

        Args:
            bucket: The InfluxDB bucket to write to.
            data_points: A list of strings, where each string is in Line Protocol format.
        """
        if not self.write_api:
            logging.error("InfluxDB write_api not initialized. Cannot write data.")
            return
        if not data_points:
            return

        max_retries = 3
        retry_delay = 2  # seconds

        for attempt in range(max_retries):
            try:
                # The ASYNCHRONOUS write_api handles batching and flushing based on WriteOptions.
                # We are just passing the prepared line protocol strings.
                self.write_api.write(bucket=bucket, org=self.influx_org, record=data_points)
                # logging.debug(f"Successfully wrote {len(data_points)} points to bucket '{bucket}'.")
                return # Success
            except InfluxDBError as e:
                logging.error(f"InfluxDBError writing to bucket '{bucket}' (attempt {attempt + 1}/{max_retries}): {e}")
                if e.response and e.response.status == 401:
                    logging.error("InfluxDB authentication error (401). Check token.")
                    break # No point retrying auth error
                if e.response and e.response.status == 404:
                    logging.error(f"InfluxDB bucket '{bucket}' not found (404).")
                    break # No point retrying if bucket doesn't exist
                # For other errors, retry after a delay
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay * (2 ** attempt)) # Exponential backoff
                else:
                    logging.error(f"Failed to write to bucket '{bucket}' after {max_retries} attempts.")
            except Exception as e:
                logging.error(f"Unexpected error writing to bucket '{bucket}' (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay * (2 ** attempt))
                else:
                    logging.error(f"Failed to write to bucket '{bucket}' after {max_retries} attempts due to unexpected error.")

    async def run_queue_consumer(self, data_queue: asyncio.Queue, bucket_name: str, stop_event: asyncio.Event):
        """
        Continuously consumes data from an asyncio.Queue and writes it to InfluxDB.
        Manages batching based on size or time.

        Args:
            data_queue: The asyncio.Queue to read data from.
                       Expected items: single LP string for trades, list of LP strings for order books.
            bucket_name: The InfluxDB bucket to write the data to.
            stop_event: An asyncio.Event to signal when to stop the consumer.
        """
        if not self.write_api:
            logging.error(f"InfluxWriter not connected. Cannot start consumer for bucket '{bucket_name}'.")
            return

        logging.info(f"Starting InfluxDB writer for bucket: {bucket_name}")
        local_batch = []
        last_flush_time = asyncio.get_event_loop().time()

        try:
            while not stop_event.is_set():
                try:
                    # Wait for an item from the queue with a timeout
                    # This allows the loop to check stop_event and flush interval periodically
                    item = await asyncio.wait_for(data_queue.get(), timeout=0.05) # 50ms timeout
                    
                    if isinstance(item, str): # Single trade LP
                        local_batch.append(item)
                    elif isinstance(item, list): # List of order book LPs
                        local_batch.extend(item)
                    else:
                        logging.warning(f"Received unexpected data type in queue for bucket {bucket_name}: {type(item)}")
                        data_queue.task_done()
                        continue

                    data_queue.task_done()

                except asyncio.TimeoutError:
                    # No item received, proceed to check flush conditions
                    pass 
                except asyncio.CancelledError:
                    logging.info(f"Writer for bucket '{bucket_name}' received cancellation.")
                    break # Exit if the task is cancelled
                except Exception as e:
                    logging.error(f"Error getting item from queue for bucket {bucket_name}: {e}")
                    # Potentially add a small sleep to prevent tight loop on persistent queue errors
                    await asyncio.sleep(0.1)
                    continue

                current_time = asyncio.get_event_loop().time()
                time_since_last_flush_ms = (current_time - last_flush_time) * 1000

                # Flush conditions
                if local_batch and \
                   (len(local_batch) >= config.WRITER_BATCH_SIZE_POINTS or \
                    time_since_last_flush_ms >= config.WRITER_FLUSH_INTERVAL_MS):
                    
                    logging.debug(f"Flushing batch to '{bucket_name}'. Size: {len(local_batch)}, Interval: {time_since_last_flush_ms:.0f}ms")
                    await self.write_batch(bucket_name, list(local_batch)) # Pass a copy
                    local_batch.clear()
                    last_flush_time = current_time
            
            # Final flush for any remaining items after stop_event is set
            if local_batch:
                logging.info(f"Flushing remaining {len(local_batch)} items from '{bucket_name}' before shutdown.")
                await self.write_batch(bucket_name, list(local_batch))
                local_batch.clear()

        except asyncio.CancelledError:
            logging.info(f"Writer for bucket '{bucket_name}' task cancelled externally.")
            # Final flush for any remaining items
            if local_batch:
                logging.info(f"Flushing remaining {len(local_batch)} items from '{bucket_name}' due to cancellation.")
                await self.write_batch(bucket_name, list(local_batch))
                local_batch.clear()
        finally:
            logging.info(f"InfluxDB writer for bucket '{bucket_name}' stopped.")

    def close(self):
        """Closes the InfluxDB client and write_api."""
        if self.write_api:
            try:
                self.write_api.close() # Flushes any pending writes and closes
                logging.info("InfluxDB write_api closed.")
            except Exception as e:
                logging.error(f"Error closing InfluxDB write_api: {e}")
            self.write_api = None
        if self.client:
            try:
                self.client.close()
                logging.info("InfluxDB client closed.")
            except Exception as e:
                logging.error(f"Error closing InfluxDB client: {e}")
            self.client = None

# Example usage (for testing, typically part of supervisor.py)
async def main_writer_test():
    # Ensure INFLUXDB_TOKEN_LOCAL is set in your environment for this test
    influx_token = os.getenv("INFLUXDB_TOKEN_LOCAL")
    if not influx_token:
        print("INFLUXDB_TOKEN_LOCAL environment variable not set. Skipping writer test.")
        return

    writer = InfluxWriter(
        influx_url=config.INFLUX_URL_LOCAL,
        influx_token=influx_token,
        influx_org=config.INFLUX_ORG
    )

    if not writer.write_api:
        print("InfluxWriter failed to initialize. Exiting test.")
        return

    test_trade_queue = asyncio.Queue()
    stop_event = asyncio.Event()

    # Start the consumer task
    writer_task = asyncio.create_task(
        writer.run_queue_consumer(test_trade_queue, config.INFLUX_BUCKET_TR, stop_event)
    )

    # Simulate putting some data
    await test_trade_queue.put("trades,exchange=test,symbol=BTC-USD,side=buy price=1.0,size=1.0 1678886400000000000")
    await test_trade_queue.put("trades,exchange=test,symbol=BTC-USD,side=sell price=2.0,size=0.5 1678886400000001000")
    
    # Test with list for order book data (if bucket is configured)
    # await test_trade_queue.put(["test_ob,level=1 price=1,amount=1 1678886400000000000", "test_ob,level=2 price=2,amount=2 1678886400000000000"])

    await asyncio.sleep(2) # Let it process

    stop_event.set() # Signal the writer to stop
    await writer_task # Wait for the writer to finish
    writer.close()
    print("Writer test completed.")

if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
    # To run this test: python -m sentinel.writers.influx_writer
    # Ensure InfluxDB is running locally and INFLUXDB_TOKEN_LOCAL is set.
    asyncio.run(main_writer_test()) 
--- End File: {relative_path} ---

